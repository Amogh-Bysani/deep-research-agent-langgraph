# Design Document: Deep Research Agent

## What I Built

I built a LangGraph pipeline that routes through planning, searching, extracting, and then draft/verify/revision. Over the course of development, I started with a CLI interface and then developed a minimal Streamlit UI for convenience during testing and demo purposes.

The primary goal of this tool is to produce web-grounded research reports with citations that actually show what sources they're from - not just "[1]" pointing to nothing, but real URLs with titles that you can verify.

I also found it interesting to implement a CoVe (Chain-of-Verification) layer for claim checking that you can optionally enable, as well as multiple report styles (default, executive, academic, bullet) that adjust the output format without changing the underlying pipeline.

**Interfaces exposed:**
- CLI (`research "query"`) for power users and scripting
- Streamlit app (`streamlit run app.py`) for interactive demo

## Architecture Decisions

### Why LangGraph?

LangGraph lets me inspect each node individually, see and follow token costs (which is valuable so I'm not stressing about how much my prompts cost), and state flows explicitly between nodes with no hidden side effects. I can add/remove nodes easily - for example, toggling CoVe on/off just adds or removes three nodes from the graph.

LangSmith integration was also a big factor. Being able to see the full trace of what happened at each step made debugging way easier than print statements everywhere.

### Node Structure
```
START
  -> plan_research
  -> run_searches
  -> select_and_extract
  -> draft_report
        -> (if CoVe enabled) -> compile_verification -> verify_claims-> revise_report
  -> END
```

**Why this flow:**
- Planner decomposes query into subquestions → better search coverage
- Extraction enforces grounding → writer only sees structured notes, not raw HTML
- Writer is constrained to notes/sources → reduces hallucination
- CoVe adds second-pass accuracy check → surfaces weak claims

### State Design

| Field | Generated By | Consumed By |
|-------|--------------|-------------|
| `query` | User input | Planner, Writer, Verifier |
| `plan` | Planner | Search node |
| `outline` | Planner | Writer (optional structure hint) |
| `search_results` | Search node | Extractor |
| `sources` | Extractor | Writer, displayed in output |
| `notes` | Extractor | Writer |
| `report_draft` | Writer | CoVe compiler, Reviser |
| `verification_spec` | CoVe compiler | Verifier |
| `verification_results` | Verifier | Reviser |
| `report` | Writer or Reviser | Final output |
| `status` | Each node | Progress tracking, debugging |

## What I Tried, What Broke, What I Changed

Over the course of the project, I started with the base implementation so I used a stub search provider with fake static data so I didn't have to deal with that when trying to figure out the folow, started with an src/agent layout and experienced some import issues
I changed it to flat agent/ at repo route, and then I also had to handle issues with JSON data from LLMs, and implemented stripping logic for markdown fences in multiple nodes as well as more explicit instruction.
Sometimes Tavily just refused to return anything, and I had to implementa  fallback in search/extraction as that would cause a crash. Eventually I realized I also had to ensure source quality / variability as early runs pulled too many single domains and I added min unique domains for this reason.

## What Worked Well

The report structure is very consistent across runs, and specific prompting leads to specific output.
Stub search mode lets me run tests fast without wasting search credits.
Report styles show configurability with minimal backend changes via prompt modification
CoVe verification protects against hallucination / minimally evaluated topics by demonstrating understanding of conflicting research.
LangSmith tracing made debugging failures easier / tracking what was happening

## Known Shortcomings & Future Direction

One thing I know that I currently haven't handled yet is a specific source quality scoring; I haven't adjusted for domain authority
Citations are writer-managed as opposed to programmatic citations
Streamlit progress is fake (simulated) and should ideally be real graph events (maybe not, I have heard that simulated progress is often better)
Evaluations are mostly structural, eventually I'd want to build some analytical parameters to judge output on metrics
No multi-hop iterating searches, gaps are revealed as opposed to searched again, could be an interesting future direction.
Extraction occasionally fails JSON parsing and falls to raw text. Made tighter, more strict prompts to fix that, and haven't experienced it, but possibly something to consider tightening.

## References

- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Chain-of-Verification (CoVe) Paper](https://arxiv.org/abs/2309.11495)
- [Tavily API](https://tavily.com/)
